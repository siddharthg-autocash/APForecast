### FILE: ./app.py

import streamlit as st
import pandas as pd
import numpy as np
import os
import io
import sys
import plotly.graph_objects as go
from datetime import datetime, timedelta

from scipy.interpolate import make_interp_spline

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

try:
    from src.apforecast.core.constants import *
    from src.apforecast.ingestion.reconciler import ingest_and_reconcile
    from src.apforecast.modeling.engine import ForecastEngine
    # vendor overrides removed
    from experiments.backtesting.core import run_walk_forward_backtest, plot_backtest_results
except ImportError as e:
    st.error(f"‚ùå Import Error: {e}")
    st.stop()

# --- PAGE CONFIG ---
st.set_page_config(page_title="APForecast Commander", layout="wide", page_icon="üí∏")
st.title("üí∏ APForecast Commander")

# --- SESSION STATE ---
if 'ledger' not in st.session_state: st.session_state['ledger'] = None
if 'forecast_df' not in st.session_state: st.session_state['forecast_df'] = None
if 'bt_clean' not in st.session_state: st.session_state['bt_clean'] = None
if 'bt_dates' not in st.session_state: st.session_state['bt_dates'] = None
if 'bt_res' not in st.session_state: st.session_state['bt_res'] = None

# ==========================================
# 0. DATA UTILITIES
# ==========================================
def smart_normalize_columns(df):
    """
    Robustly finds columns.
    PRIORITY: Forces 'Reference' (Column G) to be Vendor if present.
    """
    df.columns = [str(c).strip() for c in df.columns]
    
    rename_map = {}
    found_vendor = False
    
    for col in df.columns:
        c_lower = col.lower()
        if 'reference' in c_lower and 'bacs' not in c_lower:
            rename_map[col] = COL_VENDOR_ID
            found_vendor = True
            break

    for col in df.columns:
        if col in rename_map: continue
        c_lower = col.lower()
        
        if not found_vendor:
            if any(x in c_lower for x in ['vendor', 'payee', 'beneficiary', 'name', 'description']):
                rename_map[col] = COL_VENDOR_ID
                found_vendor = True
                continue

        if any(x in c_lower for x in ['amount', 'debit', 'payment']):
            if COL_AMOUNT not in rename_map.values():
                rename_map[col] = COL_AMOUNT
                continue
        if any(x in c_lower for x in ['check', 'num', 'doc']):
            if 'amount' not in c_lower and COL_CHECK_ID not in rename_map.values():
                rename_map[col] = COL_CHECK_ID
                continue
        if 'clear' in c_lower and 'date' in c_lower:
            rename_map[col] = 'Clear_Date'
        elif 'post' in c_lower or 'txn' in c_lower or c_lower == 'date':
            rename_map[col] = COL_POST_DATE

    df = df.rename(columns=rename_map)
    
    date_cols = [c for c in df.columns if c in [COL_POST_DATE, 'Clear_Date']]
    for dc in date_cols:
        df[dc] = pd.to_datetime(df[dc], dayfirst=True, errors='coerce')

    if COL_VENDOR_ID in df.columns:
        df[COL_VENDOR_ID] = df[COL_VENDOR_ID].astype(str).str.upper().str.strip(" -_.")

    if COL_VENDOR_ID not in df.columns:
        df[COL_VENDOR_ID] = "Unknown_Vendor"
        if not df.empty:
            st.warning(f"‚ö†Ô∏è Column Warning: Could not find Vendor/Reference column.")
    
    return df

# ==========================================
# 1. VISUALIZATION HELPERS
# ==========================================

def smooth_line_data(x, y, points=300):
    """
    Applies Cubic Spline Interpolation to smooth curves.
    Clips Y values to ensure they don't dip below 0 (for amounts) or exceed 1 (for probs).
    """
    if len(x) < 4: return x, y # Spline requires at least 4 points
    
    try:
        # Create new dense X
        x_new = np.linspace(min(x), max(x), points)
        # Create Spline
        spl = make_interp_spline(x, y, k=3)
        y_new = spl(x_new)
        
        # Clip logic: if max Y was <= 1.0 (likely probability), clip to [0, 1]
        # if max Y > 1.0 (likely money), just clip to [0, infinity]
        if max(y) <= 1.0:
            y_new = np.clip(y_new, 0, 1)
        else:
            y_new = np.clip(y_new, 0, None)
            
        return x_new, y_new
    except:
        return x, y

# (rest of visualization helpers unchanged...)
# For brevity I keep the helper functions unchanged in this file ‚Äî only vendor override plumbing removed.

# ==========================================
# 2. MAIN APP
# ==========================================
mode = st.sidebar.radio("Navigation", ["üöÄ Forecast & Intelligence", "üß™ Backtest Lab"])

if mode == "üöÄ Forecast & Intelligence":
    st.header("üöÄ Daily Cash Forecast")
    
    # --- UPDATE LEDGER FEATURE ---
    with st.expander("üîÑ Update Ledger History (Add Cleared Checks)", expanded=False):
        st.caption("Upload a file with recently cleared checks. The system will remove duplicates and add new ones to history.")
        update_file = st.file_uploader("Upload Cleared Checks File", type=['xlsx', 'csv'], key="update_upl")
        
        if update_file and st.button("Merge into History"):
            with st.spinner("Merging..."):
                try:
                    new_df = pd.read_excel(update_file) if update_file.name.endswith('.xlsx') else pd.read_csv(update_file)
                    new_df = smart_normalize_columns(new_df)
                    
                    master_path = "data/master_ledger.xlsx"
                    if os.path.exists(master_path):
                        master_df = pd.read_excel(master_path)
                    else:
                        master_df = pd.DataFrame()
                        
                    if not master_df.empty:
                        master_df = smart_normalize_columns(master_df)

                    if COL_CHECK_ID in new_df.columns and COL_CHECK_ID in master_df.columns:
                        existing_ids = set(master_df[COL_CHECK_ID].astype(str))
                        unique_new = new_df[~new_df[COL_CHECK_ID].astype(str).isin(existing_ids)]
                        
                        if not unique_new.empty:
                            updated_master = pd.concat([master_df, unique_new], ignore_index=True)
                            updated_master.to_excel(master_path, index=False)
                            st.success(f"‚úÖ Success! Added {len(unique_new)} new cleared checks to history.")
                        else:
                            st.info("‚ÑπÔ∏è No new checks found. All checks in file already exist in history.")
                    else:
                        new_df.to_excel(master_path, index=False)
                        st.success(f"‚úÖ Created new Master Ledger with {len(new_df)} rows.")
                        
                except Exception as e:
                    st.error(f"Error updating ledger: {e}")

    # --- FORECAST INPUTS ---
    with st.expander("üìÇ Run Daily Forecast", expanded=True):
        c1, c2 = st.columns(2)
        run_date = c1.date_input("Run Date", value="today")
        run_date_pd = pd.to_datetime(run_date)
        uncleared_file = c2.file_uploader("Outstanding Checks (Required)", type=['xlsx', 'csv'])
        
        if c2.button("RUN FORECAST", type="primary"):
            if uncleared_file:
                with st.spinner("Processing..."):
                    os.makedirs(f"data/raw/{run_date}", exist_ok=True)
                    with open(f"data/raw/{run_date}/uncleared_checks.xlsx", "wb") as f: f.write(uncleared_file.getbuffer())
                    
                    ledger = ingest_and_reconcile(str(run_date), run_date_pd)
                    
                    if COL_VENDOR_ID not in ledger.columns or (ledger[COL_VENDOR_ID] == 'Unknown_Vendor').all():
                        ledger = smart_normalize_columns(ledger)

                    st.session_state['ledger'] = ledger
                    # overrides removed ‚Äî engine initialized without overrides
                    engine = ForecastEngine(ledger)
                    
                    open_checks = ledger[ledger[COL_STATUS] == STATUS_OPEN].copy()
                    data = []
                    
                    for _, row in open_checks.iterrows():
                        prob = engine.predict_check(row, run_date_pd, current_date_override=run_date_pd-timedelta(days=1))
                        data.append({**row, 'Probability': prob, 'Expected_Cash': row[COL_AMOUNT]*prob})
                    
                    st.session_state['forecast_df'] = pd.DataFrame(data)
                    st.success("Done!")

    if st.session_state['forecast_df'] is not None:
        df = st.session_state['forecast_df']
        ledger = st.session_state['ledger']
        
        st.markdown("### üìä Portfolio Overview")
        c1, c2 = st.columns([1, 3])
        with c1:
            st.metric("CASH REQUIRED TODAY", f"${df['Expected_Cash'].sum():,.2f}")
            st.metric("TOTAL EXPOSURE", f"${df[COL_AMOUNT].sum():,.2f}")
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer) as writer: df.to_excel(writer, index=False)
            st.download_button("üì• Download Excel", buffer.getvalue(), f"forecast_{run_date}.xlsx")
        with c2:
            st.plotly_chart(plot_interactive_landscape(ledger, run_date_pd), use_container_width=True)

        st.markdown("---")
        st.subheader("üïµÔ∏è Vendor Intelligence")
        
        valid_vendors = sorted([v for v in df[COL_VENDOR_ID].unique() if v != "Unknown_Vendor"])
        if not valid_vendors: valid_vendors = ["Unknown_Vendor"]
        
        sel_vendor = st.selectbox("Select Vendor:", valid_vendors)
        
        # engine already created above if needed; recreate for plotting safety
        engine = ForecastEngine(ledger)
        
        if sel_vendor:
            c1, c2 = st.columns(2)
            with c1:
                if sel_vendor in engine.models['SPECIFIC']:
                    st.plotly_chart(plot_vendor_history_legacy(engine.models['SPECIFIC'][sel_vendor], sel_vendor, ledger), use_container_width=True)
                else:
                    st.info("Insufficient history for this vendor.")
            
            with c2:
                delay_days = st.slider(f"Simulate Delay for {sel_vendor}", 0, 14, 2)
                v_open = ledger[(ledger[COL_STATUS]==STATUS_OPEN) & (ledger[COL_VENDOR_ID]==sel_vendor)].copy()
                if not v_open.empty:
                    st.plotly_chart(plot_snowball_interactive(engine, v_open, run_date_pd, custom_delay=delay_days), use_container_width=True)
                    st.info(f"**Insight:** Moving the slider shows how cash flow shifts if {sel_vendor} is delayed.")
                    with st.expander("See Raw Data"):
                        v_open['Age'] = (run_date_pd - v_open[COL_POST_DATE]).dt.days
                        st.dataframe(v_open[[COL_CHECK_ID, COL_AMOUNT, COL_POST_DATE, 'Age']].sort_values('Age', ascending=False), use_container_width=True)
                else:
                    st.success("No open checks.")

# ------------------------------------------
# TAB 2: BACKTEST LAB
# ------------------------------------------
elif mode == "üß™ Backtest Lab":
    st.title("üß™ Backtest Lab")
    
    with st.expander("‚öôÔ∏è Configuration", expanded=True):
        c1, c2 = st.columns(2)
        start_dt = c1.date_input("Start Date", value=datetime.today()-timedelta(days=90))
        end_dt = c1.date_input("End Date", value=datetime.today()-timedelta(days=1))
        hist_file = c2.file_uploader("Upload Master History", type=['xlsx'])

    if hist_file and st.button("RUN GLOBAL BACKTEST", type="primary"):
        with st.spinner("Processing History..."):
            try:
                raw = pd.read_excel(hist_file)
                cols_str = "".join([str(c) for c in raw.columns])
                if "Unnamed" in cols_str or raw.shape[1] < 3:
                    raw = pd.read_excel(hist_file, header=1)
            except Exception as e:
                st.error(f"Error reading file: {e}")
                st.stop()

            clean = smart_normalize_columns(raw)
            if COL_POST_DATE in clean.columns:
                clean[COL_POST_DATE] = pd.to_datetime(clean[COL_POST_DATE], dayfirst=True, errors='coerce')
            if 'Clear_Date' in clean.columns:
                clean['Clear_Date'] = pd.to_datetime(clean['Clear_Date'], dayfirst=True, errors='coerce')
            
            # --- CALCULATE DAYS_TO_SETTLE FOR ENGINE TRAINING ---
            if 'Clear_Date' in clean.columns and COL_POST_DATE in clean.columns:
                clean['Days_to_Settle'] = (clean['Clear_Date'] - clean[COL_POST_DATE]).dt.days
                mask_cleared = clean['Clear_Date'].notna()
                clean.loc[mask_cleared, 'Status'] = 'CLEARED'
            
            st.session_state['bt_clean'] = clean
            st.session_state['bt_dates'] = (start_dt, end_dt)
            
            # overrides removed; run backtest without overrides
            res = run_walk_forward_backtest(clean, str(start_dt), str(end_dt))
            st.session_state['bt_res'] = res
            st.success("Global Backtest Complete.")

    if st.session_state['bt_res'] is not None:
        res = st.session_state['bt_res']
        
        st.markdown("### üåç Global Results")
        
        # --- CALCULATE METRICS (Including RMSE) ---
        act = res['Actual'].sum()
        pred = res['Predicted'].sum()
        var = pred - act
        
        # RMSE Logic
        mse = ((res['Predicted'] - res['Actual']) ** 2).mean()
        rmse = np.sqrt(mse)
        # Normalize by mean of actuals for % interpretation
        mean_actual = res['Actual'].mean()
        rmse_pct = (rmse / mean_actual * 100) if mean_actual > 0 else 0.0

        m1, m2, m3, m4 = st.columns(4)
        m1.metric("Actual", f"${act:,.0f}")
        m2.metric("Predicted", f"${pred:,.0f}")
        m3.metric("Variance", f"${var:,.0f}", delta=f"{(var/act)*100:.1f}%")
        m4.metric("% RMSE", f"{rmse_pct:.1f}%", help="Root Mean Squared Error Percentage. Lower is better.")
        
        st.plotly_chart(plot_backtest_results(res, "Global"), use_container_width=True)
        
        st.markdown("---")
        st.subheader("üî¨ Vendor Drill-Down")
        clean_df = st.session_state.get('bt_clean')
        
        if clean_df is not None:
            vendors = sorted([v for v in clean_df[COL_VENDOR_ID].astype(str).unique() if v != "Unknown_Vendor"])
            if not vendors: vendors = ["Unknown_Vendor"]

            c_sel, c_btn = st.columns([3, 1])
            v_sel = c_sel.selectbox("Select Vendor:", vendors)
            
            if c_btn.button(f"Run for {v_sel}"):
                s, e = st.session_state['bt_dates']
                with st.spinner(f"Simulating {v_sel}..."):
                    v_res = run_walk_forward_backtest(clean_df, str(s), str(e), vendor_filter=v_sel)
                    if not v_res.empty:
                        v_act = v_res['Actual'].sum()
                        v_pred = v_res['Predicted'].sum()
                        v_var = v_pred - v_act
                        
                        # Vendor RMSE
                        v_rmse = np.sqrt(((v_res['Predicted'] - v_res['Actual']) ** 2).mean())
                        v_mean_act = v_res['Actual'].mean()
                        v_rmse_pct = (v_rmse / v_mean_act * 100) if v_mean_act > 0 else 0.0

                        c1, c2, c3, c4 = st.columns(4)
                        c1.metric(f"{v_sel} Actual", f"${v_act:,.0f}")
                        c2.metric(f"{v_sel} Predicted", f"${v_pred:,.0f}")
                        c3.metric("Variance", f"${v_var:,.0f}")
                        c4.metric("% RMSE", f"{v_rmse_pct:.1f}%")
                        
                        st.plotly_chart(plot_backtest_results(v_res, v_sel), use_container_width=True)
                    else:
                        st.warning("No cleared checks found for this vendor in this period.")
