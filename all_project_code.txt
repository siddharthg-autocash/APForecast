
====================
FILE: ./app.py
====================
# ====================
# FILE: ./app.py
# ====================
import streamlit as st
import pandas as pd
import numpy as np
import os
import io
import sys
import plotly.graph_objects as go
from datetime import datetime, timedelta

# --- PATH SETUP ---
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# --- IMPORTS ---
try:
    from src.apforecast.core.constants import *
    from src.apforecast.ingestion.reconciler import ingest_and_reconcile
    from src.apforecast.modeling.engine import ForecastEngine
    from src.apforecast.core.config_loader import load_vendor_overrides
    from experiments.backtesting.core import run_walk_forward_backtest, plot_backtest_results
except ImportError as e:
    st.error(f"‚ùå Import Error: {e}")
    st.stop()

# --- PAGE CONFIG ---
st.set_page_config(page_title="APForecast Commander", layout="wide", page_icon="üí∏")
st.title("üí∏ APForecast Commander")

# --- SESSION STATE ---
if 'ledger' not in st.session_state: st.session_state['ledger'] = None
if 'forecast_df' not in st.session_state: st.session_state['forecast_df'] = None
if 'bt_clean' not in st.session_state: st.session_state['bt_clean'] = None
if 'bt_dates' not in st.session_state: st.session_state['bt_dates'] = None
if 'bt_res' not in st.session_state: st.session_state['bt_res'] = None

# ==========================================
# 0. DATA UTILITIES (The "Column Hunter")
# ==========================================
def smart_normalize_columns(df):
    """
    Robustly finds columns.
    PRIORITY: Forces 'Reference' (Column G) to be Vendor if present.
    """
    # 1. Clean headers: strip whitespace and convert to string
    df.columns = [str(c).strip() for c in df.columns]
    
    rename_map = {}
    found_vendor = False
    
    # --- PRE-SCAN: The "Reference" Force ---
    # We check for 'Reference' specifically before the main loop to ensure 
    # generic keywords like 'Name' in other columns don't steal the spot.
    for col in df.columns:
        c_lower = col.lower()
        if 'reference' in c_lower and 'bacs' not in c_lower:
            rename_map[col] = COL_VENDOR_ID
            found_vendor = True
            break 

    # 2. Main Search Logic 
    for col in df.columns:
        if col in rename_map: continue
        c_lower = col.lower()
        
        # Vendor Fallback (only if Reference wasn't found)
        if not found_vendor:
            if any(x in c_lower for x in ['vendor', 'payee', 'beneficiary', 'name', 'description']):
                rename_map[col] = COL_VENDOR_ID
                found_vendor = True
                continue

        # Other Columns
        if any(x in c_lower for x in ['amount', 'debit', 'payment']):
            if COL_AMOUNT not in rename_map.values():
                rename_map[col] = COL_AMOUNT
                continue
        if any(x in c_lower for x in ['check', 'num', 'doc']):
            if 'amount' not in c_lower and COL_CHECK_ID not in rename_map.values():
                rename_map[col] = COL_CHECK_ID
                continue
        if 'clear' in c_lower and 'date' in c_lower:
            rename_map[col] = 'Clear_Date'
        elif 'post' in c_lower or 'txn' in c_lower or c_lower == 'date':
            rename_map[col] = COL_POST_DATE

    # 3. Apply Renames
    df = df.rename(columns=rename_map)
    
    # --- FIX 1: DATE PARSING (Day First) ---
    # We explicitly tell pandas to try 'dayfirst=True' for formats like 22-07-2025
    date_cols = [c for c in df.columns if c in [COL_POST_DATE, 'Clear_Date']]
    for dc in date_cols:
        df[dc] = pd.to_datetime(df[dc], dayfirst=True, errors='coerce')

    # --- FIX 2: VENDOR NORMALIZATION ---
    # Consolidate "American Hose -" and "AMERICAN HOSE" into one ID
    if COL_VENDOR_ID in df.columns:
        df[COL_VENDOR_ID] = df[COL_VENDOR_ID].astype(str).str.upper().str.strip(" -_.")

    # 4. Final Safety Check
    if COL_VENDOR_ID not in df.columns:
        df[COL_VENDOR_ID] = "Unknown_Vendor"
        if not df.empty:
            st.warning(f"‚ö†Ô∏è Column Warning: Could not find Vendor/Reference column.")
    
    return df

# ==========================================
# 1. VISUALIZATION HELPERS
# ==========================================

def plot_vendor_history_legacy(model, vendor_name, ledger):
    """ Classic History Plot """
    history = ledger[(ledger[COL_VENDOR_ID] == vendor_name) & (ledger['Status'] == 'CLEARED')].copy()
    if history.empty: return None

    history['Days_Taken'] = (history['Clear_Date'] - history[COL_POST_DATE]).dt.days
    history = history[history['Days_Taken'] >= 0]
    if history.empty: return None

    daily_stats = history.groupby('Days_Taken').agg({
        COL_AMOUNT: 'sum',
        COL_CHECK_ID: list,
        COL_VENDOR_ID: 'count'
    }).rename(columns={COL_VENDOR_ID: 'Count'}).reset_index()

    daily_stats['Probability'] = daily_stats['Count'] / daily_stats['Count'].sum()
    daily_stats['Label'] = daily_stats[COL_AMOUNT].apply(lambda x: f"${x/1000:.0f}k" if x >= 1000 else f"${int(x)}")

    def build_hover(row):
        ids = sorted([str(x) for x in row[COL_CHECK_ID]])[:5]
        id_str = "<br>‚Ä¢ " + "<br>‚Ä¢ ".join(ids)
        return (f"<b>Day {row['Days_Taken']}</b><br>Prob: {row['Probability']:.1%}<br>"
                f"Vol: <b>${row[COL_AMOUNT]:,.0f}</b><br>IDs:{id_str}")
    daily_stats['Hover_Text'] = daily_stats.apply(build_hover, axis=1)

    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=daily_stats['Days_Taken'], y=daily_stats['Probability'],
        text=daily_stats['Label'], textposition='outside',
        marker_color='#E67E22', opacity=0.6,
        hovertext=daily_stats['Hover_Text'], hoverinfo="text",
        name="Hist. Probability"
    ))
    
    x_range = np.arange(0, daily_stats['Days_Taken'].max() + 5)
    fig.add_trace(go.Scatter(
        x=x_range, y=[model.cdf(x) for x in x_range],
        name="Cumulative (CDF)", mode='lines',
        line=dict(color='#154360', width=3), yaxis="y2"
    ))

    fig.update_layout(
        title=f"<b>Payment Profile: {vendor_name}</b>",
        yaxis=dict(title="Probability", tickformat=".0%"),
        yaxis2=dict(title="Cumulative %", overlaying="y", side="right", showgrid=False),
        template="plotly_white", height=450, showlegend=False
    )
    return fig

def plot_snowball_interactive(engine, open_checks, run_date, custom_delay=0):
    """ Forecast Plot """
    fig = go.Figure()
    dates = [run_date + timedelta(days=i) for i in range(14)]
    
    base_cash = []
    hover_texts = []
    
    for d in dates:
        day_total = 0
        contributors = []
        for _, row in open_checks.iterrows():
            # The Engine now handles Specific -> Global fallback internally
            prob = engine.predict_check(row, d, current_date_override=run_date - timedelta(days=1))
            sim_age = (d - row[COL_POST_DATE]).days
            
            expected = row[COL_AMOUNT] * prob
            
            if expected > 0.50: 
                day_total += expected
                contributors.append((row[COL_CHECK_ID], row[COL_AMOUNT], sim_age, prob))
        
        base_cash.append(day_total)
        
        # Tooltip Logic
        contributors.sort(key=lambda x: x[1], reverse=True)
        top_list = contributors[:20] 
        
        hover_html = f"<b>{d.strftime('%b-%d')}</b><br>Expected: <b>${day_total:,.0f}</b><br><br>Checks Contributing:"
        for cid, amt, age, p in top_list:
            hover_html += f"<br>‚Ä¢ #{cid} (${amt:,.0f}) | Age: {age}d | Prob: {p:.0%}"
        if len(contributors) > 20:
            hover_html += f"<br><i>...and {len(contributors)-20} more</i>"
        hover_texts.append(hover_html)

    # Base Case Bar
    fig.add_trace(go.Bar(
        x=[d.strftime('%b-%d') for d in dates], 
        y=base_cash,
        name="Expected Cash Flow",
        marker_color='#F5B041', opacity=0.8,
        text=[f"${v/1000:.0f}k" if v > 1000 else "" for v in base_cash],
        textposition='outside',
        hovertext=hover_texts, hoverinfo="text"
    ))

    # Scenario Line
    if custom_delay > 0:
        delay_cash = []
        for d in dates:
            sim_date = d - timedelta(days=custom_delay)
            day_total = 0
            for _, row in open_checks.iterrows():
                if sim_date < run_date: prob = 0
                else:
                    # The Engine handles the fallback here too
                    prob = engine.predict_check(row, sim_date, current_date_override=run_date - timedelta(days=1))
                            
                day_total += (row[COL_AMOUNT] * prob)
            delay_cash.append(day_total)
            
        fig.add_trace(go.Scatter(
            x=[d.strftime('%b-%d') for d in dates], 
            y=delay_cash,
            name=f"Scenario: Delay +{custom_delay} Days",
            mode='lines+markers',
            line=dict(width=3, color='#27AE60'),
            marker=dict(size=6)
        ))

    fig.update_layout(
        title="<b>Cash Flow Forecast</b>",
        xaxis_title="Future Date", yaxis_title="Expected Outflow ($)",
        template="plotly_white", height=500, hovermode="x unified",
        legend=dict(x=0.01, y=0.99, bgcolor='rgba(255,255,255,0.8)')
    )
    return fig

def plot_interactive_landscape(ledger, run_date):
    """ Colorful Jitter """
    open_checks = ledger[ledger[COL_STATUS] == STATUS_OPEN].copy()
    if open_checks.empty: return None
    open_checks['Age'] = (run_date - open_checks[COL_POST_DATE]).dt.days
    
    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=open_checks['Age'], y=open_checks[COL_AMOUNT], mode='markers',
        marker=dict(
            size=open_checks[COL_AMOUNT].apply(lambda x: np.log(x)*3 if x>0 else 5),
            color=open_checks['Age'], colorscale='Portland', showscale=True,
            line=dict(width=1, color='DarkSlateGrey')
        ),
        text=open_checks[COL_VENDOR_ID],
        hovertemplate="<b>%{text}</b><br>$%{y:,.2f}<br>%{x} days old"
    ))
    fig.update_layout(
        title="<b>Outstanding Check Landscape</b><br><sup>Color = Age | Size = Amount</sup>",
        xaxis_title="Days Since Posted", yaxis_title="Amount ($)",
        template="plotly_white", height=450
    )
    return fig

# ==========================================
# 2. MAIN APP
# ==========================================
mode = st.sidebar.radio("Navigation", ["üöÄ Forecast & Intelligence", "üß™ Backtest Lab"])

if mode == "üöÄ Forecast & Intelligence":
    st.header("üöÄ Daily Cash Forecast")
    
    # --- UPDATE LEDGER FEATURE ---
    with st.expander("üîÑ Update Ledger History (Add Cleared Checks)", expanded=False):
        st.caption("Upload a file with recently cleared checks. The system will remove duplicates and add new ones to history.")
        update_file = st.file_uploader("Upload Cleared Checks File", type=['xlsx', 'csv'], key="update_upl")
        
        if update_file and st.button("Merge into History"):
            with st.spinner("Merging..."):
                try:
                    # 1. Load the new file
                    new_df = pd.read_excel(update_file) if update_file.name.endswith('.xlsx') else pd.read_csv(update_file)
                    new_df = smart_normalize_columns(new_df)
                    
                    # 2. Load the existing Master Ledger (assuming it sits in data/master.xlsx)
                    # NOTE: Adjust path to where your master file actually lives
                    master_path = "data/master_ledger.xlsx"
                    if os.path.exists(master_path):
                        master_df = pd.read_excel(master_path)
                    else:
                        master_df = pd.DataFrame()
                        
                    if not master_df.empty:
                        master_df = smart_normalize_columns(master_df)

                    # 3. Deduplicate
                    if COL_CHECK_ID in new_df.columns and COL_CHECK_ID in master_df.columns:
                        existing_ids = set(master_df[COL_CHECK_ID].astype(str))
                        
                        # Filter for rows where Check ID is NOT in existing
                        unique_new = new_df[~new_df[COL_CHECK_ID].astype(str).isin(existing_ids)]
                        
                        if not unique_new.empty:
                            # Append
                            updated_master = pd.concat([master_df, unique_new], ignore_index=True)
                            updated_master.to_excel(master_path, index=False)
                            st.success(f"‚úÖ Success! Added {len(unique_new)} new cleared checks to history.")
                        else:
                            st.info("‚ÑπÔ∏è No new checks found. All checks in file already exist in history.")
                    else:
                        # Fallback if starting fresh
                        new_df.to_excel(master_path, index=False)
                        st.success(f"‚úÖ Created new Master Ledger with {len(new_df)} rows.")
                        
                except Exception as e:
                    st.error(f"Error updating ledger: {e}")

    # --- FORECAST INPUTS ---
    with st.expander("üìÇ Run Daily Forecast", expanded=True):
        c1, c2 = st.columns(2)
        run_date = c1.date_input("Run Date", value="today")
        run_date_pd = pd.to_datetime(run_date)
        uncleared_file = c2.file_uploader("Outstanding Checks (Required)", type=['xlsx', 'csv'])
        
        if c2.button("RUN FORECAST", type="primary"):
            if uncleared_file:
                with st.spinner("Processing..."):
                    os.makedirs(f"data/raw/{run_date}", exist_ok=True)
                    with open(f"data/raw/{run_date}/uncleared_checks.xlsx", "wb") as f: f.write(uncleared_file.getbuffer())
                    
                    ledger = ingest_and_reconcile(str(run_date), run_date_pd)
                    
                    if COL_VENDOR_ID not in ledger.columns or (ledger[COL_VENDOR_ID] == 'Unknown_Vendor').all():
                        ledger = smart_normalize_columns(ledger)

                    st.session_state['ledger'] = ledger
                    overrides = load_vendor_overrides()
                    engine = ForecastEngine(ledger, overrides)
                    
                    open_checks = ledger[ledger[COL_STATUS] == STATUS_OPEN].copy()
                    data = []
                    
                    # --- CLEAN PREDICTION LOOP (ENGINE HANDLES FALLBACK) ---
                    for _, row in open_checks.iterrows():
                        # The Engine handles the "Specific -> Global" fallback internally now.
                        prob = engine.predict_check(row, run_date_pd, current_date_override=run_date_pd-timedelta(days=1))
                        data.append({**row, 'Probability': prob, 'Expected_Cash': row[COL_AMOUNT]*prob})
                    
                    st.session_state['forecast_df'] = pd.DataFrame(data)
                    st.success("Done!")

    if st.session_state['forecast_df'] is not None:
        df = st.session_state['forecast_df']
        ledger = st.session_state['ledger']
        
        st.markdown("### üìä Portfolio Overview")
        c1, c2 = st.columns([1, 3])
        with c1:
            st.metric("CASH REQUIRED TODAY", f"${df['Expected_Cash'].sum():,.2f}")
            st.metric("TOTAL EXPOSURE", f"${df[COL_AMOUNT].sum():,.2f}")
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer) as writer: df.to_excel(writer, index=False)
            st.download_button("üì• Download Excel", buffer.getvalue(), f"forecast_{run_date}.xlsx")
        with c2:
            st.plotly_chart(plot_interactive_landscape(ledger, run_date_pd), use_container_width=True)

        st.markdown("---")
        st.subheader("üïµÔ∏è Vendor Intelligence")
        
        valid_vendors = sorted([v for v in df[COL_VENDOR_ID].unique() if v != "Unknown_Vendor"])
        if not valid_vendors: valid_vendors = ["Unknown_Vendor"]
        
        sel_vendor = st.selectbox("Select Vendor:", valid_vendors)
        
        overrides = load_vendor_overrides()
        engine = ForecastEngine(ledger, overrides)
        
        if sel_vendor:
            c1, c2 = st.columns(2)
            with c1:
                if sel_vendor in engine.models['SPECIFIC']:
                    st.plotly_chart(plot_vendor_history_legacy(engine.models['SPECIFIC'][sel_vendor], sel_vendor, ledger), use_container_width=True)
                else:
                    st.info("Insufficient history for this vendor.")
            
            with c2:
                delay_days = st.slider(f"Simulate Delay for {sel_vendor}", 0, 14, 2)
                v_open = ledger[(ledger[COL_STATUS]==STATUS_OPEN) & (ledger[COL_VENDOR_ID]==sel_vendor)].copy()
                if not v_open.empty:
                    st.plotly_chart(plot_snowball_interactive(engine, v_open, run_date_pd, custom_delay=delay_days), use_container_width=True)
                    st.info(f"**Insight:** Moving the slider shows how cash flow shifts if {sel_vendor} is delayed.")
                    with st.expander("See Raw Data"):
                        v_open['Age'] = (run_date_pd - v_open[COL_POST_DATE]).dt.days
                        st.dataframe(v_open[[COL_CHECK_ID, COL_AMOUNT, COL_POST_DATE, 'Age']].sort_values('Age', ascending=False), use_container_width=True)
                else:
                    st.success("No open checks.")

# ------------------------------------------
# TAB 2: BACKTEST LAB
# ------------------------------------------
elif mode == "üß™ Backtest Lab":
    st.title("üß™ Backtest Lab")
    
    with st.expander("‚öôÔ∏è Configuration", expanded=True):
        c1, c2 = st.columns(2)
        start_dt = c1.date_input("Start Date", value=datetime.today()-timedelta(days=90))
        end_dt = c1.date_input("End Date", value=datetime.today()-timedelta(days=1))
        hist_file = c2.file_uploader("Upload Master History", type=['xlsx'])

    if hist_file and st.button("RUN GLOBAL BACKTEST", type="primary"):
        with st.spinner("Processing History..."):
            try:
                raw = pd.read_excel(hist_file)
                cols_str = "".join([str(c) for c in raw.columns])
                if "Unnamed" in cols_str or raw.shape[1] < 3:
                    raw = pd.read_excel(hist_file, header=1)
            except Exception as e:
                st.error(f"Error reading file: {e}")
                st.stop()

            clean = smart_normalize_columns(raw)
            if COL_POST_DATE in clean.columns:
                clean[COL_POST_DATE] = pd.to_datetime(clean[COL_POST_DATE], dayfirst=True, errors='coerce')
            if 'Clear_Date' in clean.columns:
                clean['Clear_Date'] = pd.to_datetime(clean['Clear_Date'], dayfirst=True, errors='coerce')
            
            # --- CALCULATE DAYS_TO_SETTLE FOR ENGINE TRAINING ---
            if 'Clear_Date' in clean.columns and COL_POST_DATE in clean.columns:
                clean['Days_to_Settle'] = (clean['Clear_Date'] - clean[COL_POST_DATE]).dt.days
                # Ensure status is cleared where appropriate
                mask_cleared = clean['Clear_Date'].notna()
                clean.loc[mask_cleared, 'Status'] = 'CLEARED'
            
            st.session_state['bt_clean'] = clean
            st.session_state['bt_dates'] = (start_dt, end_dt)
            
            overrides = load_vendor_overrides()
            res = run_walk_forward_backtest(clean, str(start_dt), str(end_dt), overrides)
            st.session_state['bt_res'] = res
            st.success("Global Backtest Complete.")

    if st.session_state['bt_res'] is not None:
        res = st.session_state['bt_res']
        
        st.markdown("### üåç Global Results")
        m1, m2, m3 = st.columns(3)
        act = res['Actual'].sum()
        pred = res['Predicted'].sum()
        var = pred - act
        m1.metric("Actual", f"${act:,.0f}")
        m2.metric("Predicted", f"${pred:,.0f}")
        m3.metric("Variance", f"${var:,.0f}", delta=f"{(var/act)*100:.1f}%")
        
        st.plotly_chart(plot_backtest_results(res, "Global"), use_container_width=True)
        
        st.markdown("---")
        st.subheader("üî¨ Vendor Drill-Down")
        clean_df = st.session_state.get('bt_clean')
        
        if clean_df is not None:
            vendors = sorted([v for v in clean_df[COL_VENDOR_ID].astype(str).unique() if v != "Unknown_Vendor"])
            if not vendors: vendors = ["Unknown_Vendor"]

            c_sel, c_btn = st.columns([3, 1])
            v_sel = c_sel.selectbox("Select Vendor:", vendors)
            
            if c_btn.button(f"Run for {v_sel}"):
                s, e = st.session_state['bt_dates']
                overrides = load_vendor_overrides()
                with st.spinner(f"Simulating {v_sel}..."):
                    v_res = run_walk_forward_backtest(clean_df, str(s), str(e), overrides, vendor_filter=v_sel)
                    if not v_res.empty:
                        v_act = v_res['Actual'].sum()
                        v_pred = v_res['Predicted'].sum()
                        v_var = v_pred - v_act
                        c1, c2, c3 = st.columns(3)
                        c1.metric(f"{v_sel} Actual", f"${v_act:,.0f}")
                        c2.metric(f"{v_sel} Predicted", f"${v_pred:,.0f}")
                        c3.metric("Variance", f"${v_var:,.0f}")
                        st.plotly_chart(plot_backtest_results(v_res, v_sel), use_container_width=True)
                    else:
                        st.warning("No cleared checks found for this vendor in this period.")

====================
FILE: ./experiments/backtesting/core.py
====================
# experiments/backtesting/core.py
import pandas as pd
import numpy as np
from datetime import timedelta
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from src.apforecast.core.constants import *
from src.apforecast.modeling.engine import ForecastEngine

def run_walk_forward_backtest(full_ledger, start_date, end_date, overrides, vendor_filter=None):
    """
    Rolling Backtest Logic.
    - Added 'vendor_filter': If set, runs backtest ONLY for that vendor.
    """
    
    # 1. FORCE STATUS Logic (Fix missing 'CLEARED' labels)
    full_ledger = full_ledger.copy()
    valid_dates = pd.to_datetime(full_ledger['Clear_Date'], errors='coerce').notna()
    full_ledger.loc[valid_dates, 'Status'] = 'CLEARED'
    
    # 2. FILTER BY VENDOR (If requested)
    if vendor_filter:
        print(f"üéØ Filtered Backtest for Vendor: {vendor_filter}")
        full_ledger = full_ledger[full_ledger[COL_VENDOR_ID] == vendor_filter].copy()
        if full_ledger.empty:
            return pd.DataFrame()

    results = []
    current_date = pd.to_datetime(start_date)
    end_date_pd = pd.to_datetime(end_date)
    
    print(f"üîÑ Starting Walk-Forward Backtest ({start_date} to {end_date})...")
    
    cumulative_error = 0.0

    while current_date <= end_date_pd:
        # --- TRAIN ---
        history_mask = (full_ledger['Status'] == 'CLEARED') & (full_ledger['Clear_Date'] < current_date)
        training_data = full_ledger[history_mask].copy()
        
        # Lower threshold for specific vendor backtests
        min_history = 5 if vendor_filter else 50
        
        if len(training_data) < min_history:
            # print(f"‚ö†Ô∏è Skipping {current_date.date()}: Not enough history.")
            current_date += timedelta(days=1)
            continue

        engine = ForecastEngine(training_data, overrides)
        
        # --- IDENTIFY OPEN CHECKS ---
        open_mask = (full_ledger[COL_POST_DATE] <= current_date) & (full_ledger['Clear_Date'] >= current_date)
        daily_open_checks = full_ledger[open_mask].copy()
        
        # --- PREDICT ---
        predicted_cash = 0.0
        flagged_volume = 0.0 
        
        for _, row in daily_open_checks.iterrows():
            age = (current_date - row[COL_POST_DATE]).days
            if age > 45:
                flagged_volume += row[COL_AMOUNT]
                continue
            
            prob = engine.predict_check(
                row, 
                current_date, 
                current_date_override=current_date - timedelta(days=1)
            )
            predicted_cash += (row[COL_AMOUNT] * prob)
            
        # --- ACTUALS ---
        actual_mask = (full_ledger['Clear_Date'] == current_date)
        actual_cash = full_ledger[actual_mask][COL_AMOUNT].sum()
        
        # --- METRICS ---
        residual = actual_cash - predicted_cash
        cumulative_error += residual
        error_pct = (residual / actual_cash) if actual_cash > 10 else 0.0
        
        results.append({
            'Date': current_date,
            'Predicted': round(predicted_cash, 2),
            'Actual': round(actual_cash, 2),
            'Residual ($)': round(residual, 2),
            'Error %': round(error_pct * 100, 1),
            'Cum. Error': round(cumulative_error, 2),
            'Flagged (>45d)': round(flagged_volume, 2)
        })
        
        current_date += timedelta(days=1)
        
    return pd.DataFrame(results)

def plot_backtest_results(df, title_prefix="Global"):
    """
    Generates the Perfect 2-Panel Dashboard.
    """
    if df.empty:
        return go.Figure()

    fig = make_subplots(
        rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.1,
        subplot_titles=(f"{title_prefix} Cash Flow: Predicted vs Actual", "Daily Deviation (Residual Error)"),
        row_heights=[0.7, 0.3]
    )
    
    # Panel 1: Main
    fig.add_trace(go.Scatter(
        x=df['Date'], y=df['Actual'], name='Actual Cash',
        line=dict(color='#27AE60', width=2), fill='tozeroy', opacity=0.2
    ), row=1, col=1)
    
    fig.add_trace(go.Scatter(
        x=df['Date'], y=df['Predicted'], name='Model Prediction',
        mode='lines+markers', line=dict(color='#154360', width=3), marker=dict(size=4)
    ), row=1, col=1)
    
    # Panel 2: Errors
    fig.add_trace(go.Bar(
        x=df['Date'], y=df['Residual ($)'], name='Error ($)',
        marker_color='crimson', opacity=0.6,
        hovertemplate='%{y:$.2f}<br>Error: %{customdata}%',
        customdata=df['Error %']
    ), row=2, col=1)
    
    fig.update_layout(
        title=f"<b>{title_prefix} Backtest Report</b>",
        template="plotly_white", height=700, hovermode="x unified"
    )
    fig.add_hline(y=0, line_dash="dash", line_color="black", row=2, col=1)
    
    return fig

====================
FILE: ./experiments/backtesting/cli.py
====================
# experiments/backtesting/cli.py
import sys
import os
import argparse
import pandas as pd
from datetime import datetime

# --- PATH HACK (To import 'src') ---
current_file = os.path.abspath(__file__)
backtesting_dir = os.path.dirname(current_file) 
experiments_dir = os.path.dirname(backtesting_dir) 
project_root = os.path.dirname(experiments_dir) 
if project_root not in sys.path:
    sys.path.insert(0, project_root)
# -----------------------------------

# Split imports correctly
from src.apforecast.core.constants import * from src.apforecast.core.config_loader import load_vendor_overrides
from experiments.backtesting.core import run_walk_forward_backtest, plot_backtest_results

def normalize_columns(df):
    """
    Maps various Excel header styles to the internal standard names.
    """
    column_map = {
        # Clear Date Variations
        "Clear Date": "Clear_Date",
        "ClearDate": "Clear_Date",
        "Cleared Date": "Clear_Date",
        
        # Post Date Variations
        "Post Date": COL_POST_DATE,
        "PostDate": COL_POST_DATE,
        "Date": COL_POST_DATE,
        
        # Vendor Variations
        "Vendor ID": COL_VENDOR_ID,
        "Vendor Name": COL_VENDOR_ID,
        "Vendor": COL_VENDOR_ID,
        "Payee": COL_VENDOR_ID,
        
        # Amount Variations
        "Amount": COL_AMOUNT,
        "Check Amount": COL_AMOUNT,
        "Transaction Amount": COL_AMOUNT,
        
        # Check ID Variations
        "Check Number": COL_CHECK_ID,
        "Check #": COL_CHECK_ID,
        "Reference": COL_CHECK_ID
    }
    
    df = df.rename(columns=column_map)
    
    # --- CRITICAL FIX: Assign Dummy Vendor BEFORE Validation ---
    if COL_VENDOR_ID not in df.columns:
        print("‚ö†Ô∏è WARNING: No Vendor ID found. Assigning 'Unknown_Vendor' to all rows.")
        df[COL_VENDOR_ID] = "Unknown_Vendor"
    # -----------------------------------------------------------
    
    required = ["Clear_Date", COL_POST_DATE, COL_AMOUNT, COL_VENDOR_ID]
    missing = [c for c in required if c not in df.columns]
    
    if missing:
        raise ValueError(f"‚ùå Missing critical columns: {missing}\n   Found: {list(df.columns)}")
        
    return df

def main():
    parser = argparse.ArgumentParser(description="üß™ Rolling Backtest Verification")
    parser.add_argument("--start", type=str, required=True, help="Start Date (YYYY-MM-DD)")
    parser.add_argument("--end", type=str, required=True, help="End Date (YYYY-MM-DD)")
    parser.add_argument("--source", type=str, required=True, help="Path to historical file")
    
    args = parser.parse_args()
    
    print("\nüß™ INITIALIZING ROLLING BACKTEST LAB")
    print("=" * 60)
    
    try:
        print(f"üìÇ Loading History from: {args.source}")
        df = pd.read_excel(args.source)
        df = normalize_columns(df)
        
        # Convert Dates
        df[COL_POST_DATE] = pd.to_datetime(df[COL_POST_DATE])
        df["Clear_Date"] = pd.to_datetime(df["Clear_Date"])
        
        # Ensure Status exists
        if COL_STATUS not in df.columns:
            df[COL_STATUS] = 'CLEARED'

        # === NEW: DATA DIAGNOSTICS ===
        min_date = df['Clear_Date'].min()
        max_date = df['Clear_Date'].max()
        count = len(df)
        
        print(f"‚úÖ Loaded {count} records.")
        print("-" * 30)
        print(f"üìÖ EARLIEST Clear Date: {min_date.date()}")
        print(f"üìÖ LATEST   Clear Date: {max_date.date()}")
        print("-" * 30)
        
        # Validation Logic
        start_dt = pd.to_datetime(args.start)
        
        # Count how many records exist BEFORE the requested start date
        prior_history = df[df['Clear_Date'] < start_dt]
        print(f"üîç History available before {args.start}: {len(prior_history)} records")
        
        if len(prior_history) < 50:
            print("\n‚ùå CRITICAL ISSUE: Not enough history to train the model.")
            print(f"   You requested start date: {args.start}")
            print(f"   But your data only starts at: {min_date.date()}")
            print("   üëâ ACTION: Please choose a '--start' date at least 3 months AFTER your earliest data.")
            return
        # ==============================

    except Exception as e:
        print(f"‚ùå Error loading file: {e}")
        return

    # Run Backtest
    try:
        overrides = load_vendor_overrides()
        metrics_df = run_walk_forward_backtest(df, args.start, args.end, overrides)
        
        if metrics_df.empty:
            print("‚ùå No results generated.")
            return

        total_pred = metrics_df['Predicted'].sum()
        total_act = metrics_df['Actual'].sum()
        total_err = total_pred - total_act
        
        print("\nüìä SUMMARY RESULTS")
        print("-" * 30)
        print(f"üìÖ Period: {args.start} to {args.end}")
        print(f"üí∞ Total Predicted: ${total_pred:,.2f}")
        print(f"üí∞ Total Actual:    ${total_act:,.2f}")
        print(f"üìâ Net Variance:    ${total_err:,.2f} ({total_err/total_act:.1%})")
        
    except Exception as e:
        print(f"‚ùå Error during simulation: {e}")
        import traceback
        traceback.print_exc()
        return

    # Save Outputs
    output_dir = os.path.join(project_root, "experiments", "results")
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    excel_path = os.path.join(output_dir, f"backtest_metrics_{timestamp}.xlsx")
    metrics_df.to_excel(excel_path, index=False)
    
    fig = plot_backtest_results(metrics_df)
    html_path = os.path.join(output_dir, f"backtest_plot_{timestamp}.html")
    fig.write_html(html_path)
    
    print(f"\n‚úÖ Verification Complete.")
    print(f"üìù Excel: {excel_path}")
    print(f"üìà Graph: {html_path}\n")

if __name__ == "__main__":
    main()

====================
FILE: ./src/apforecast/main.py
====================
# src/apforecast/main.py

# python3 -m src.apforecast.main --date 16-01-2026

import argparse
import pandas as pd
import os
from src.apforecast.core.constants import *
from src.apforecast.core.config_loader import load_vendor_overrides
from src.apforecast.ingestion.reconciler import ingest_and_reconcile
from src.apforecast.modeling.engine import ForecastEngine
from src.apforecast.reporting.dashboard import generate_report
# --- CHANGE 3: Import Visuals ---
from src.apforecast.reporting.visuals import plot_model_curves 

def main():
    parser = argparse.ArgumentParser(description="APForecast System")
    parser.add_argument("--date", type=str, required=True, help="Run date (DD-MM-YYYY)")
    args = parser.parse_args()
    
    run_date_str = args.date
    run_date = pd.to_datetime(run_date_str, format="%d-%m-%Y")
    
    print(f"--- Starting APForecast for {run_date_str} ---")

    # 1. Ingest & Reconcile (Now handles .xlsx)
    ledger = ingest_and_reconcile(run_date_str, run_date)
    
    # 2. Load Config Overrides
    overrides = load_vendor_overrides()
    
    # 3. Initialize Engine (Training)
    engine = ForecastEngine(ledger, overrides)
    
    # --- CHANGE 4: Generate Reference Graphs ---
    # This will create the PNGs for every vendor/cohort
    plot_model_curves(engine.models, run_date_str)
    
    # 4. Forecast Loop
    open_checks = ledger[ledger[COL_STATUS] == STATUS_OPEN].copy()
    print(f"Forecasting for {len(open_checks)} open checks...")
    
    results = []
    for _, row in open_checks.iterrows():
        prob = engine.predict_check(row, run_date)
        expected_cash = row[COL_AMOUNT] * prob
        
        results.append({
            COL_CHECK_ID: row[COL_CHECK_ID],
            COL_VENDOR_ID: row[COL_VENDOR_ID],
            COL_AMOUNT: row[COL_AMOUNT],
            COL_POST_DATE: row[COL_POST_DATE],
            'Probability': round(prob, 4),
            'Expected_Cash': round(expected_cash, 2)
        })
        
    forecast_df = pd.DataFrame(results)
    
    # 5. Report
    os.makedirs(REPORTS_DIR, exist_ok=True)
    generate_report(forecast_df, run_date_str)
    
    print("--- Process Complete ---")

if __name__ == "__main__":
    main()

====================
FILE: ./src/apforecast/core/config_loader.py
====================
# src/apforecast/core/config_loader.py
import pandas as pd
from src.apforecast.core.constants import CONFIG_FILE_PATH

def load_vendor_overrides():
    """
    Reads the Excel config file and returns a dictionary of active overrides.
    Key: Vendor_ID
    Value: Dict of strategy parameters
    """
    try:
        df = pd.read_excel(CONFIG_FILE_PATH)
        # Filter for active rules only
        active_rules = df[df['Active'] == True]
        
        overrides = {}
        for _, row in active_rules.iterrows():
            overrides[row['Vendor_ID']] = {
                'Strategy': row['Strategy'],
                'Param_1': row['Param_1'],
                'Param_2': row['Param_2']
            }
        return overrides
    except FileNotFoundError:
        print("Warning: No config file found. Proceeding with defaults.")
        return {}

====================
FILE: ./src/apforecast/core/constants.py
====================
# src/apforecast/core/constants.py

# Paths
DATA_DIR = "data"
RAW_DIR = f"{DATA_DIR}/raw"
PROCESSED_DIR = f"{DATA_DIR}/processed"
CONFIG_DIR = f"{DATA_DIR}/config"
REPORTS_DIR = "reports"

MASTER_LEDGER_PATH = f"{PROCESSED_DIR}/master_ledger.parquet"
CONFIG_FILE_PATH = f"{CONFIG_DIR}/vendor_strategy_overrides.xlsx"

# --- COLUMN MAPPING (CRITICAL) ---
# Format: "Your_File_Column_Name": "System_Name"
# UPDATE THE LEFT SIDE to match your CSV headers exactly!
# src/apforecast/core/constants.py

COLUMN_MAP = {
    # "YOUR FILE HEADER"   : "SYSTEM INTERNAL NAME" (Do not change right side!)
    "Check #"              : "Check_ID",
    "Reference"            : "Vendor_ID",
    "Amount"               : "Amount",
    "Post Date"            : "Post_Date",  # Assuming your file header IS "Post_Date"
    "Cleared Date"           : "Clear_Date"  # Assuming your file header IS "Clear_Date"
}

# Internal System Column Names (Do Not Change These)
COL_CHECK_ID = "Check_ID"
COL_VENDOR_ID = "Vendor_ID"
COL_AMOUNT = "Amount"
COL_POST_DATE = "Post_Date"
COL_CLEAR_DATE = "Clear_Date"
COL_STATUS = "Status"
COL_DAYS_TO_SETTLE = "Days_to_Settle"

# Statuses
STATUS_OPEN = "OPEN"
STATUS_CLEARED = "CLEARED"
STATUS_VOID = "VOID"

# Cohorts
THRESHOLD_SMALL = 10000
THRESHOLD_LARGE = 50000
COHORT_SMALL = "STABLE_SMALL"
COHORT_MEDIUM = "VOLATILE_MED"
COHORT_LARGE = "LAZY_GIANT"

# Strategies
STRAT_FIXED_LAG = "FIXED_LAG"
STRAT_WEEKDAY = "WEEKDAY"
STRAT_EXACT_DATE = "EXACT_DATE"
STRAT_HOLD = "HOLD"
STRAT_PROB_OVERRIDE = "PROBABILITY_OVERRIDE"
STRAT_DEFAULT = "DEFAULT"

====================
FILE: ./src/apforecast/reporting/dashboard.py
====================
# src/apforecast/reporting/dashboard.py
import pandas as pd
from src.apforecast.core.constants import REPORTS_DIR

def generate_report(forecast_df, run_date_str):
    """
    forecast_df has columns: [Check_ID, Vendor, Amount, Probability, Expected_Cash]
    """
    filename = f"{REPORTS_DIR}/forecast_{run_date_str}.xlsx"
    
    writer = pd.ExcelWriter(filename, engine='xlsxwriter')
    
    # Summary Sheet
    total_exposure = forecast_df['Amount'].sum()
    expected_outflow = forecast_df['Expected_Cash'].sum()
    
    summary_data = {
        'Metric': ['Total Open AP', 'Expected Cash Outflow (Today)'],
        'Value': [total_exposure, expected_outflow]
    }
    pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)
    
    # Detail Sheet
    forecast_df.sort_values(by='Probability', ascending=False, inplace=True)
    forecast_df.to_excel(writer, sheet_name='Check_Details', index=False)
    
    writer.close()
    print(f"Report saved: {filename}")

====================
FILE: ./src/apforecast/reporting/evolution.py
====================
# src/apforecast/reporting/evolution.py
import pandas as pd
import plotly.graph_objects as go
from datetime import timedelta
from src.apforecast.core.constants import COL_AMOUNT, COL_VENDOR_ID, COL_CHECK_ID, COL_POST_DATE

def simulate_snowball_effect(engine, open_checks, start_date, simulation_days=3, horizon=14, save_plot=False, vendor_filter=None):
    """
    Generates Snowball Chart + Returns Data.
    Drill-Down Data is STRICTLY sorted by AGE (Oldest First).
    """
    
    if vendor_filter and vendor_filter != "ALL":
        open_checks = open_checks[open_checks[COL_VENDOR_ID] == vendor_filter].copy()
        title_suffix = f"for {vendor_filter}"
    else:
        title_suffix = "(All Vendors)"

    if open_checks.empty:
        return None, {}

    fig = go.Figure()
    LINE_COLORS = ['#154360', '#C0392B', '#27AE60', '#8E44AD'] 
    BAR_COLOR = '#E67E22'
    
    breakdown_data = {} 

    for i in range(simulation_days):
        sim_current_date = start_date + timedelta(days=i)
        scenario_label = f"Scenario: Delay {i} Day{'s' if i > 0 else ''}"
        breakdown_data[scenario_label] = {}
        
        daily_amounts = {}
        hover_texts = {} 
        
        plot_horizon = horizon + simulation_days 
        
        for d in range(plot_horizon):
            target_date = start_date + timedelta(days=d)
            if target_date < sim_current_date: continue
                
            day_contributors = []
            total_expected = 0
            
            for _, row in open_checks.iterrows():
                # Conditional Prob: P(Clear Today | Alive Yesterday)
                p_cum_target = engine.predict_check(row, target_date, current_date_override=sim_current_date)
                p_cum_yesterday = engine.predict_check(row, target_date - timedelta(days=1), current_date_override=sim_current_date)
                p_marginal = max(0, p_cum_target - p_cum_yesterday)
                
                expected_cash = row[COL_AMOUNT] * p_marginal
                
                if expected_cash > 0.01:
                    total_expected += expected_cash
                    age_at_payment = (target_date - row[COL_POST_DATE]).days
                    
                    day_contributors.append({
                        'Check_ID': row[COL_CHECK_ID],
                        'Vendor': row[COL_VENDOR_ID],
                        'Amount_Full': row[COL_AMOUNT],
                        'Expected_Cash': expected_cash,
                        'Age_Days': age_at_payment
                    })
            
            if total_expected > 0.01:
                daily_amounts[target_date] = total_expected
                
                # SORTING: Oldest Checks First
                df_contrib = pd.DataFrame(day_contributors)
                df_contrib = df_contrib.sort_values(by='Age_Days', ascending=False)
                breakdown_data[scenario_label][target_date] = df_contrib
                
                # Custom Hover
                limit = 20
                top_n = df_contrib.head(limit)
                hover_str = f"<b>Total: ${total_expected:,.0f}</b><br><br>Oldest Contributors:"
                for _, c in top_n.iterrows():
                    hover_str += f"<br>‚Ä¢ #{c['Check_ID']} (Age: {c['Age_Days']}d) | ${c['Expected_Cash']:,.0f}"
                if len(df_contrib) > limit:
                    hover_str += f"<br><i>...and {len(df_contrib)-limit} more</i>"
                hover_texts[target_date] = hover_str

        dates = sorted(list(daily_amounts.keys()))
        values = [daily_amounts[d] for d in dates]
        hovers = [hover_texts[d] for d in dates]
        
        if i == 0:
            fig.add_trace(go.Bar(
                x=dates, y=values, name="Expected Cash Flow (Base Case)",
                marker_color=BAR_COLOR, opacity=0.5,
                hovertext=hovers, hoverinfo="text+x+name"
            ))

        fig.add_trace(go.Scatter(
            x=dates, y=values, mode='lines+markers', name=scenario_label,
            line=dict(color=LINE_COLORS[i % len(LINE_COLORS)], width=3),
            marker=dict(size=6),
            hovertext=hovers, hoverinfo="text+x+name"
        ))

    fig.update_layout(
        title=dict(text=f"<b>Snowball Forecast: {title_suffix}</b>", font=dict(size=20)),
        xaxis_title="Future Date", yaxis_title="Expected Outflow ($)",
        yaxis_tickprefix="$", hovermode="closest", height=500
    )
    
    return fig, breakdown_data

====================
FILE: ./src/apforecast/reporting/interactive.py
====================
# src/apforecast/reporting/interactive.py
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
from src.apforecast.core.constants import *

def plot_interactive_outstanding_analysis(open_checks, run_date):
    """
    Generates an Interactive Jitter + Box Plot.
    User can hover over individual dots to see Check Age & Details.
    """
    if open_checks.empty:
        return None

    # 1. Prepare Data
    df = open_checks.copy()
    
    # Calculate Age
    df['Age_Days'] = (run_date - df[COL_POST_DATE]).dt.days
    df['Age_Days'] = df['Age_Days'].apply(lambda x: max(0, x)) # No negative age
    
    # Create a truncated Vendor Name for cleaner X-axis
    df['Vendor_Short'] = df[COL_VENDOR_ID].astype(str).str.slice(0, 15)
    
    # Create Hover Text
    df['Hover_Info'] = (
        "<b>Vendor:</b> " + df[COL_VENDOR_ID].astype(str) + "<br>" +
        "<b>Check ID:</b> " + df[COL_CHECK_ID].astype(str) + "<br>" +
        "<b>Amount:</b> $" + df[COL_AMOUNT].apply(lambda x: "{:,.2f}".format(x)) + "<br>" +
        "<b>Age:</b> " + df['Age_Days'].astype(str) + " days old"
    )

    # 2. Build the Plot (Strip Plot = Jitter)
    # FIX: Removed 'color_continuous_scale' from here to prevent the error
    fig = px.strip(
        df, 
        x='Vendor_Short', 
        y=COL_AMOUNT, 
        color='Age_Days',
        custom_data=[COL_VENDOR_ID, COL_CHECK_ID, 'Age_Days', COL_AMOUNT],
        title=f"Outstanding Checks Analysis (Jitter + Box)<br><sup>Hover over dots to see specific check details</sup>"
    )
    
    # FIX: Apply the 'Red-Yellow-Green' color scale safely via layout
    fig.update_layout(coloraxis=dict(colorscale='RdYlGn_r'))

    # 3. Add Box Plot Layer (for Distribution stats)
    fig.add_trace(
        go.Box(
            y=df[COL_AMOUNT],
            x=df['Vendor_Short'],
            name="Distribution",
            boxpoints=False, # We already have points from px.strip
            fillcolor='rgba(0,0,0,0)', # Transparent fill
            line=dict(color='gray', width=1.5),
            hoverinfo='skip' # Don't clutter hover with box stats
        )
    )

    # 4. Styling
    fig.update_traces(
        hovertemplate="<b>%{customdata[0]}</b><br>Amount: $%{y:,.0f}<br>Age: %{customdata[2]} days<br>Check ID: %{customdata[1]}"
    )
    
    fig.update_layout(
        xaxis_title="Vendor",
        yaxis_title="Check Amount ($)",
        yaxis_tickprefix="$",
        height=600,
        plot_bgcolor="white",
        hoverlabel=dict(bgcolor="white", font_size=14)
    )
    
    fig.update_xaxes(showgrid=True, gridcolor='#eee')
    fig.update_yaxes(showgrid=True, gridcolor='#eee')

    return fig

====================
FILE: ./src/apforecast/reporting/visuals.py
====================
# src/apforecast/reporting/visuals.py
import plotly.graph_objects as go
import pandas as pd
import numpy as np
from src.apforecast.core.constants import *

def plot_box_jitter_history(ledger, vendor_name):
    """
    Plots Historical Payment Behavior using a Box Plot + Jitter Points.
    - X Axis: Days Taken to Clear
    - Jitter Points: Individual Checks (Hover for details)
    """
    # Filter History
    history = ledger[
        (ledger[COL_VENDOR_ID] == vendor_name) & 
        (ledger['Status'] == 'CLEARED')
    ].copy()
    
    if history.empty:
        return None

    # Calculate Days
    history['Days_Taken'] = (history['Clear_Date'] - history[COL_POST_DATE]).dt.days
    history = history[history['Days_Taken'] >= 0]
    
    fig = go.Figure()

    # 1. Jitter Points (The Individual Checks)
    fig.add_trace(go.Box(
        x=history['Days_Taken'],
        name=vendor_name,
        boxpoints='all',          # Show all points
        jitter=0.5,               # Spread them out
        pointpos=-1.8,            # Place points under the box
        marker=dict(
            color='#E67E22',
            size=6,
            opacity=0.6
        ),
        line=dict(color='#2C3E50'),
        fillcolor='rgba(230, 126, 34, 0.2)', # Orange tint
        text=history.apply(
            lambda r: f"Check #{r[COL_CHECK_ID]}<br>${r[COL_AMOUNT]:,.2f}<br>Posted: {r[COL_POST_DATE].strftime('%Y-%m-%d')}", 
            axis=1
        ),
        hoverinfo='text'
    ))

    fig.update_layout(
        title=f"<b>Historical Behavior: {vendor_name}</b><br><sup>Distribution of Days Taken to Clear</sup>",
        xaxis_title="Days to Clear",
        yaxis_title="",
        showlegend=False,
        height=400,
        template="plotly_white",
        margin=dict(l=40, r=40, t=80, b=40)
    )
    return fig

def plot_forecast_distribution(open_checks, run_date):
    """
    Visualizes the Forecast Risks using Box + Jitter.
    - X Axis: Vendor Name (or 'Portfolio' if global)
    - Y Axis: Projected Days Outstanding (Age + Forecast)
    """
    # Calculate 'Current Age' for visualization context
    open_checks['Current_Age'] = (run_date - open_checks[COL_POST_DATE]).dt.days
    
    fig = go.Figure()
    
    # We plot the 'Current Age' distribution to show Risk Profile
    fig.add_trace(go.Box(
        y=open_checks['Current_Age'],
        name="Outstanding Portfolio",
        boxpoints='all',
        jitter=0.5,
        pointpos=-1.8,
        marker=dict(color='#3498DB', size=5, opacity=0.5),
        line=dict(color='#154360'),
        fillcolor='rgba(52, 152, 219, 0.2)',
        text=open_checks.apply(
            lambda r: f"<b>{r[COL_VENDOR_ID]}</b><br>Check #{r[COL_CHECK_ID]}<br>${r[COL_AMOUNT]:,.2f}<br>Age: {r['Current_Age']} days", 
            axis=1
        ),
        hoverinfo='text'
    ))

    fig.update_layout(
        title="<b>Outstanding Risk Profile</b><br><sup>Age Distribution of Unpaid Checks (Higher = Older/Riskier)</sup>",
        yaxis_title="Current Age (Days)",
        xaxis_title="",
        showlegend=False,
        height=500,
        template="plotly_white"
    )
    return fig

====================
FILE: ./src/apforecast/ingestion/reconciler.py
====================
# src/apforecast/ingestion/reconciler.py
import pandas as pd
import os
import glob
from src.apforecast.core.constants import *
from src.apforecast.ingestion.loader import load_file_smart

def ingest_and_reconcile(date_str, run_date):
    # 1. Load Brain (Master Ledger)
    if os.path.exists(MASTER_LEDGER_PATH):
        ledger = pd.read_parquet(MASTER_LEDGER_PATH)
    else:
        # --- INITIALIZATION ---
        # Look for ANY file in history folder
        hist_files = glob.glob(f"{DATA_DIR}/raw/history/*")
        if hist_files:
            # Pick the first file found (e.g., your Cleared Checks.csv)
            hist_path = hist_files[0]
            print(f"Initializing Master Ledger from: {hist_path}")
            
            ledger = load_file_smart(hist_path)
            
            # Ensure required columns exist
            required = [COL_CHECK_ID, COL_VENDOR_ID, COL_POST_DATE, COL_CLEAR_DATE]
            if not all(col in ledger.columns for col in required):
                raise ValueError(f"History file missing columns! Found: {ledger.columns}. Check constants.py mapping.")

            ledger[COL_POST_DATE] = pd.to_datetime(ledger[COL_POST_DATE])
            ledger[COL_CLEAR_DATE] = pd.to_datetime(ledger[COL_CLEAR_DATE])
            
            # Calculate history logic
            ledger[COL_DAYS_TO_SETTLE] = (ledger[COL_CLEAR_DATE] - ledger[COL_POST_DATE]).dt.days
            ledger[COL_STATUS] = STATUS_CLEARED
        else:
            raise FileNotFoundError("No history file found in data/raw/history/")

    # 2. Load Daily Inputs
    daily_folder = f"{RAW_DIR}/{date_str}"
    
    # Find the "Outstanding" file (Uncleared)
    input_files = glob.glob(f"{daily_folder}/*")
    
    issued_checks = pd.DataFrame()
    bank_cleared = pd.DataFrame()

    for f in input_files:
        if "Outstanding" in f or "uncleared" in f.lower():
            print(f"Loading Outstanding Checks: {f}")
            issued_checks = load_file_smart(f)
        elif "Cleared" in f or "bank" in f.lower():
            print(f"Loading Cleared Checks: {f}")
            bank_cleared = load_file_smart(f)

    # Standardize IDs
    if COL_CHECK_ID in ledger.columns:
        ledger[COL_CHECK_ID] = ledger[COL_CHECK_ID].astype(str)

    # 3a. Reconciliation (Matches)
    if not bank_cleared.empty and COL_CHECK_ID in bank_cleared.columns:
        bank_cleared[COL_CHECK_ID] = bank_cleared[COL_CHECK_ID].astype(str)
        cleared_ids = bank_cleared[COL_CHECK_ID].tolist()
        
        mask_cleared = ledger[COL_CHECK_ID].isin(cleared_ids)
        ledger.loc[mask_cleared, COL_STATUS] = STATUS_CLEARED
        
        # Update dates
        if COL_CLEAR_DATE in bank_cleared.columns:
            date_map = bank_cleared.set_index(COL_CHECK_ID)[COL_CLEAR_DATE].to_dict()
            ledger.loc[mask_cleared, COL_CLEAR_DATE] = ledger.loc[mask_cleared, COL_CHECK_ID].map(date_map)
            ledger.loc[mask_cleared, COL_CLEAR_DATE] = pd.to_datetime(ledger.loc[mask_cleared, COL_CLEAR_DATE])
        else:
            ledger.loc[mask_cleared, COL_CLEAR_DATE] = run_date
            
        ledger.loc[mask_cleared, COL_DAYS_TO_SETTLE] = (
            ledger.loc[mask_cleared, COL_CLEAR_DATE] - ledger.loc[mask_cleared, COL_POST_DATE]
        ).dt.days

    # 3b. Add New Uncleared Checks
    if not issued_checks.empty and COL_CHECK_ID in issued_checks.columns:
        issued_checks[COL_CHECK_ID] = issued_checks[COL_CHECK_ID].astype(str)
        
        # Filter duplicates
        existing_ids = set(ledger[COL_CHECK_ID])
        new_checks = issued_checks[~issued_checks[COL_CHECK_ID].isin(existing_ids)].copy()
        
        if not new_checks.empty:
            new_checks[COL_STATUS] = STATUS_OPEN
            new_checks[COL_POST_DATE] = pd.to_datetime(new_checks[COL_POST_DATE])
            new_checks[COL_DAYS_TO_SETTLE] = None 
            
            # Align columns before concat
            ledger = pd.concat([ledger, new_checks], ignore_index=True)

    # 4. Save
    os.makedirs(PROCESSED_DIR, exist_ok=True)
    ledger.to_parquet(MASTER_LEDGER_PATH)
    print("Master Ledger updated.")
    return ledger

====================
FILE: ./src/apforecast/ingestion/loader.py
====================
# src/apforecast/ingestion/loader.py
import pandas as pd
import os
from src.apforecast.core.constants import COLUMN_MAP

def load_file_smart(filepath):
    """
    Reads CSV or Excel. Renames columns based on constants.py map.
    """
    if not os.path.exists(filepath):
        raise FileNotFoundError(f"File not found: {filepath}")

    # Auto-detect format
    if filepath.endswith('.csv'):
        df = pd.read_csv(filepath)
    else:
        # Defaults to Excel for xlsx, xls
        df = pd.read_excel(filepath)

    # Normalize Columns (Map User headers to System headers)
    # We invert the map to rename: {User_Col: System_Col}
    rename_dict = {v: k for k, v in COLUMN_MAP.items()} 
    # Wait, the map in constants is {User_Name : System_Name} ?? 
    # Let's assume the user edits constants.py to be: "Check Number": "Check_ID"
    # So we simply pass COLUMN_MAP.
    
    # Actually, standardizing the map in constants:
    # Let's rely on the user putting correct keys in COLUMN_MAP.
    # Current CONSTANTS structure: {"User_Header": "System_Internal_Name"}
    
    df.rename(columns=COLUMN_MAP, inplace=True)
    
    # Strip whitespace from string columns
    df.columns = [c.strip() for c in df.columns]
    
    return df

====================
FILE: ./src/apforecast/modeling/cohorts.py
====================
# src/apforecast/modeling/cohorts.py
from src.apforecast.core.constants import *

def determine_cohort(amount):
    if amount < THRESHOLD_SMALL:
        return COHORT_SMALL
    elif amount < THRESHOLD_LARGE:
        return COHORT_MEDIUM
    else:
        return COHORT_LARGE

====================
FILE: ./src/apforecast/modeling/probability.py
====================
# src/apforecast/modeling/probability.py
import numpy as np
import pandas as pd

class BayesianModel:
    def __init__(self, days_data):
        self.sorted_data = np.sort(days_data)
        self.n = len(days_data)
        
        # FIX: Store the maximum historical day observed.
        # This defines the "Settlement Max" for Step 1.
        self.max_observed_days = self.sorted_data[-1] if self.n > 0 else 0

    def cdf(self, t):
        """Empirical Cumulative Distribution Function"""
        if self.n == 0: return 0.0
        # searchsorted returns where t would fit in the list
        count = np.searchsorted(self.sorted_data, t, side='right')
        return count / self.n

    def predict_survival_probability(self, current_age, window=1):
        """
        Calculates P(Clear in Window | Alive at current_age).
        """
        # STEP 1: CHECK <= SETTLEMENT MAX
        # If the check is older than our entire history for this vendor, 
        # we return None. This signals the Engine to resort to Step 2 (Global).
        if self.n == 0 or current_age > self.max_observed_days:
            return None

        cdf_t = self.cdf(current_age)
        
        # EDGE CASE: If CDF is 1.0 (100% cleared), we can't divide by (1-1).
        # We return 0.0 because strictly speaking, locally, 
        # there is 0% probability remaining in *this* specific history.
        if cdf_t >= 0.9999:
            return 0.0

        cdf_t_window = self.cdf(current_age + window)
        
        # Standard conditional probability formula
        # P(A|B) = P(A and B) / P(B)
        # Probability of clearing next / Probability of surviving until now
        prob = (cdf_t_window - cdf_t) / (1.0 - cdf_t)
        
        return max(0.0, min(1.0, prob))

====================
FILE: ./src/apforecast/modeling/engine.py
====================
# src/apforecast/modeling/engine.py
import pandas as pd
import numpy as np
from src.apforecast.core.constants import *
from src.apforecast.modeling.probability import BayesianModel
from src.apforecast.modeling.cohorts import determine_cohort

class ForecastEngine:
    def __init__(self, ledger, overrides):
        self.ledger = ledger
        self.overrides = overrides
        self.models = self._train_models()

    def _train_models(self):
        models = {'SPECIFIC': {}, 'GLOBAL': {}}
        cleared = self.ledger[self.ledger[COL_STATUS] == STATUS_CLEARED]

        # 1. Train Specific Models
        vendor_counts = cleared[COL_VENDOR_ID].value_counts()
        valid_vendors = vendor_counts[vendor_counts >= 5].index
        for v_id in valid_vendors:
            data = cleared[cleared[COL_VENDOR_ID] == v_id][COL_DAYS_TO_SETTLE].values
            models['SPECIFIC'][v_id] = BayesianModel(data)

        # 2. Train Global Cohorts
        for cohort in [COHORT_SMALL, COHORT_MEDIUM, COHORT_LARGE]:
            mask = cleared[COL_AMOUNT].apply(determine_cohort) == cohort
            data = cleared[mask][COL_DAYS_TO_SETTLE].values
            models['GLOBAL'][cohort] = BayesianModel(data)
            
        return models
    
    def predict_check(self, check_row, forecast_date, current_date_override=None):
        """
        Returns a probability related to `forecast_date`.

        Behavior:
        - If `current_date_override` is provided:
            Return P(Clear on or before forecast_date | still unpaid at current_date_override).
            (This is a conditional cumulative probability. Marginal for a single day can be
            computed by differencing two calls with adjacent target dates.)
        - If `current_date_override` is NOT provided:
            Return unconditional P(Clear on or before forecast_date) (simple empirical CDF).
        """
        vendor_id = check_row[COL_VENDOR_ID]
        amount = check_row[COL_AMOUNT]
        post_date = pd.to_datetime(check_row[COL_POST_DATE])

        # compute ages (in days)
        forecast_age = (pd.to_datetime(forecast_date) - post_date).days

        if current_date_override is not None:
            current_age = (pd.to_datetime(current_date_override) - post_date).days
        else:
            current_age = None

        # 0. USER OVERRIDES (Top Priority)
        if vendor_id in self.overrides:
            rule = self.overrides[vendor_id]
            strategy = rule.get('Strategy')
            p1 = rule.get('Param_1')
            p2 = rule.get('Param_2')

            # For overrides, use forecast-based semantics:
            # e.g., FIXED_LAG = probability 1 when forecast_age >= p1
            if strategy == STRAT_HOLD:
                return 0.0
            if strategy == STRAT_PROB_OVERRIDE:
                return float(p1)
            if strategy == STRAT_FIXED_LAG:
                return 1.0 if forecast_age >= int(p1) else 0.0
            if strategy == STRAT_EXACT_DATE:
                return 1.0 if pd.to_datetime(forecast_date).normalize() == pd.to_datetime(p1).normalize() else 0.0
            if strategy == STRAT_WEEKDAY:
                target_day = str(p1).title()
                return float(p2) if pd.to_datetime(forecast_date).day_name() == target_day else 0.1

        # Helper to get a model for vendor / cohort
        def get_model_for(vendor, amt):
            if vendor in self.models['SPECIFIC']:
                return self.models['SPECIFIC'][vendor]
            cohort = determine_cohort(amt)
            return self.models['GLOBAL'].get(cohort)

        model = get_model_for(vendor_id, amount)

        # If no model at all, return 0
        if model is None or model.n == 0:
            return 0.0

        # CASE A: Conditional (we know state 'alive' at current_age)
        if current_age is not None:
            # If forecast is at-or-before the current age, no new probability
            if forecast_age <= current_age:
                return 0.0

            # If current_age beyond observed history -> can't compute conditional from specific model
            if current_age > model.max_observed_days:
                # allow fallback to GLOBAL if SPECIFIC unavailable; try global model
                # (get_model_for already tried GLOBAL for cohort when SPECIFIC missing)
                # but if this model is SPECIFIC and it's out-of-range, try global cohort model:
                cohort_model = self.models['GLOBAL'].get(determine_cohort(amount))
                if cohort_model and cohort_model.n > 0 and current_age <= cohort_model.max_observed_days:
                    model = cohort_model
                else:
                    return 0.0

            # Compute conditional cumulative:
            cdf_current = model.cdf(current_age)
            cdf_forecast = model.cdf(forecast_age)

            denom = 1.0 - cdf_current
            if denom <= 1e-12:
                return 0.0

            prob_conditional_cum = (cdf_forecast - cdf_current) / denom
            return max(0.0, min(1.0, prob_conditional_cum))

        # CASE B: Unconditional cumulative (no current_date_override provided)
        # just return empirical CDF at forecast_age
        return max(0.0, min(1.0, model.cdf(forecast_age)))

